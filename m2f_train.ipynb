{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d10f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -U -q transformers[torch] evaluate timm albumentations accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50396430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ad32938a610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import datasets\n",
    "import requests\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import huggingface_hub\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os\n",
    "import matplotlib.patches as mpatches\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    MaskFormerImageProcessor,\n",
    "    AutoImageProcessor,\n",
    "    MaskFormerForInstanceSegmentation,\n",
    ")\n",
    "from transformers import MaskFormerConfig\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c26144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'CC Clyr Can', 1: 'TT Temp Tar', 2: 'RAT'}\n"
     ]
    }
   ],
   "source": [
    "# For Can RAT ontology\n",
    "\n",
    "names= [\"CC Clyr Can\", \"TT Temp Tar\", \"RAT\"]\n",
    "\n",
    "categories = {\n",
    "    \"CC Clyr Can\": 0,  # No need for 'unlabeled' at index 0\n",
    "    \"TT Temp Tar\": 1,\n",
    "    \"RAT\": 2\n",
    "}\n",
    "\n",
    "id2label = {0: \"CC Clyr Can\", 1: \"TT Temp Tar\", 2: \"RAT\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3d814a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory //content//data//Training_Data//Jan_2026//Can_and_RAT/hf_mask2former_can_rat_dataset not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-974491375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlocal_dataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'//content//data//Training_Data//Jan_2026//Can_and_RAT'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hf_mask2former_can_rat_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_to_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory {dataset_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m     if fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) and fs.isfile(\n\u001b[1;32m   1468\u001b[0m         \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_STATE_JSON_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory //content//data//Training_Data//Jan_2026//Can_and_RAT/hf_mask2former_can_rat_dataset not found"
     ]
    }
   ],
   "source": [
    "# Pull datasets from local directories for images and dataset\n",
    "import os\n",
    "local_image_dir = r'//content//data//Training_Data//Jan_2026//Can_and_RAT//images'\n",
    "local_dataset_dir = r'//content//data//Training_Data//Jan_2026//Can_and_RAT'\n",
    "\n",
    "hf_dataset = load_from_disk(os.path.join(local_dataset_dir, \"hf_mask2former_can_rat_dataset\"))\n",
    "train_ds = hf_dataset['train']\n",
    "val_ds = hf_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from google drive if stored there\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy dataset from Drive to local runtime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Source (Google Drive)\n",
    "drive_dataset_dir = '/content/drive/MyDrive/Training_Data/Jan_2026/Can_and_RAT'\n",
    "\n",
    "# Destination (local runtime - fast!)\n",
    "local_dataset_dir = '/content/Training_Data/Jan_2026/Can_and_RAT'\n",
    "\n",
    "# Copy to local\n",
    "print(\"Copying dataset from Google Drive to local runtime...\")\n",
    "shutil.copytree(drive_dataset_dir, local_dataset_dir)\n",
    "print(\"✓ Copy complete!\")\n",
    "\n",
    "# Now load from LOCAL path (fast)\n",
    "hf_dataset = load_from_disk(os.path.join(local_dataset_dir, \"hf_mask2former_can_rat_dataset\"))\n",
    "train_ds = hf_dataset['train']\n",
    "val_ds = hf_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f85782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset: datasets.Dataset, n: int = 5):\n",
    "    \"\"\"\n",
    "    Displays 'n' samples from the dataset.\n",
    "    ----\n",
    "    Args:\n",
    "      - dataset: The dataset which should contain 'pixel_values' and 'label' in its items.\n",
    "      - n (int): Number of samples to display.\n",
    "\n",
    "    \"\"\"\n",
    "    if n > len(dataset):\n",
    "        raise ValueError(\"n is larger than the dataset size\")\n",
    "\n",
    "    fig, axs = plt.subplots(n, 2, figsize=(10, 5 * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        sample = dataset[i]\n",
    "        image = np.array(sample[\"pixel_values\"])\n",
    "        label = np.array(sample[\"label\"])\n",
    "\n",
    "        axs[i, 0].imshow(image)\n",
    "        axs[i, 0].set_title(\"Image\")\n",
    "        axs[i, 0].axis(\"off\")\n",
    "\n",
    "        axs[i, 1].imshow(image)\n",
    "        axs[i, 1].imshow(label/len(names), cmap=\"nipy_spectral\", alpha=0.5)\n",
    "        axs[i, 1].set_title(\"Segmentation Map\")\n",
    "        axs[i, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(train_ds, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74779189",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = MaskFormerImageProcessor(\n",
    "    ignore_index=255, #this was originally set to zero which meant background was ignored!\n",
    "    do_reduce_labels=False,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=False,\n",
    ")\n",
    "ade_mean = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ade_std = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=512),  # Resize the image to have the smallest side be 512 while maintaining aspect ratio.\n",
    "        A.RandomCrop(width=512, height=512),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=ade_mean, std=ade_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=512, height=512),\n",
    "        A.Normalize(mean=ade_mean, std=ade_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationDataInput:\n",
    "    original_image: np.ndarray\n",
    "    transformed_image: np.ndarray\n",
    "    original_segmentation_map: np.ndarray\n",
    "    transformed_segmentation_map: np.ndarray\n",
    "\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset: datasets.Dataset, transform: Any) -> None:\n",
    "        \"\"\"\n",
    "        Dataset for Semantic Segmentation.\n",
    "        ----\n",
    "        Args:\n",
    "          - dataset: A dataset containing images and segmentation maps.\n",
    "          - transform: A transformation function to apply to the images and segmentation maps.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        sample = self.dataset[idx]\n",
    "        original_image = np.array(sample[\"pixel_values\"])\n",
    "        original_segmentation_map = np.array(sample[\"label\"])\n",
    "\n",
    "        transformed = self.transform(\n",
    "            image=original_image, mask=original_segmentation_map\n",
    "        )\n",
    "        transformed_image = transformed[\"image\"].transpose(\n",
    "            2, 0, 1\n",
    "        )  # Transpose to channel-first format\n",
    "        transformed_segmentation_map = transformed[\"mask\"]\n",
    "\n",
    "        return SegmentationDataInput(\n",
    "            original_image=original_image,\n",
    "            transformed_image=transformed_image,\n",
    "            original_segmentation_map=original_segmentation_map,\n",
    "            transformed_segmentation_map=transformed_segmentation_map,\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch: SegmentationDataInput) -> dict:\n",
    "    original_images = [sample.original_image for sample in batch]\n",
    "    transformed_images = [sample.transformed_image for sample in batch]\n",
    "    original_segmentation_maps = [sample.original_segmentation_map for sample in batch]\n",
    "    transformed_segmentation_maps = [\n",
    "        sample.transformed_segmentation_map for sample in batch\n",
    "    ]\n",
    "\n",
    "    preprocessed_batch = preprocessor(\n",
    "        transformed_images,\n",
    "        segmentation_maps=transformed_segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    preprocessed_batch[\"original_images\"] = original_images\n",
    "    preprocessed_batch[\"original_segmentation_maps\"] = original_segmentation_maps\n",
    "\n",
    "    return preprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SemanticSegmentationDataset(train_ds, transform=train_transform)\n",
    "val_dataset = SemanticSegmentationDataset(val_ds, transform=train_transform)\n",
    "test_dataset = SemanticSegmentationDataset(val_ds, transform=test_transform)#Need a separate test set eventually\n",
    "\n",
    "# Prepare Dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 1 - spatial dimensions\n",
    "sample = next(iter(train_dataloader))\n",
    "print(\n",
    "    {\n",
    "        key: value[0].shape if isinstance(value, list) else value.shape\n",
    "        for key, value in sample.items()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 2 - denormalization and visualization\n",
    "def denormalize_image(image, mean, std):\n",
    "    \"\"\"\n",
    "    Denormalizes a normalized image.\n",
    "    ----\n",
    "    Args:\n",
    "     - image (numpy.ndarray): The normalized image.\n",
    "     - mean (list or numpy.ndarray): The mean used for normalization.\n",
    "     - std (list or numpy.ndarray): The standard deviation used for normalization.\n",
    "\n",
    "    \"\"\"\n",
    "    unnormalized_image = (image * std[:, None, None]) + mean[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).numpy().astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    return unnormalized_image\n",
    "\n",
    "\n",
    "denormalized_image = denormalize_image(sample[\"pixel_values\"][0], ade_mean, ade_std)\n",
    "pil_image = Image.fromarray(denormalized_image)\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 3 - mask labels consistent to class labels\n",
    "labels = [id2label[label] for label in sample[\"class_labels\"][0].tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 4 - mask visualization\n",
    "def visualize_mask(sample, labels, label_name):\n",
    "    print(f\"Category: {label_name}\")\n",
    "    idx = labels.index(label_name)\n",
    "\n",
    "    visual_mask = (sample[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n",
    "    return Image.fromarray(visual_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdfa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check 5 - visualize_mask\n",
    "visualize_mask(sample, labels, labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64870c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "\n",
    "#output_dir = r'/content/drive/My Drive/AI Models/Mask2Former'\n",
    "#model_iteration = os.path.join(output_dir,'27March_3')\n",
    "#processor = AutoImageProcessor.from_pretrained(model_iteration)\n",
    "#model = MaskFormerForInstanceSegmentation.from_pretrained(\n",
    " #   model_iteration, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    "#)\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "#    model_iteration, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    "#)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-coco-instance\")\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-ade-semantic\")\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "   \"facebook/mask2former-swin-base-ade-semantic\", id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-coco\")\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\n",
    "    \"facebook/maskformer-swin-base-coco\", id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d465bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.pixel_level_module.encoder.encoder.layers[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd62675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the backbone and pixel decoder\n",
    "# pixel level module contains both the backbone and the pixel decoder\n",
    "for param in model.model.pixel_level_module.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Confirm that the parameters are correctly frozen\n",
    "for name, param in model.model.pixel_level_module.named_parameters():\n",
    "    assert not param.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24160300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_gradually(model, num_backbone_stages_to_unfreeze=2):\n",
    "    \"\"\"\n",
    "    Unfreezes the last few blocks of the backbone and pixel decoder gradually.\n",
    "\n",
    "    Args:\n",
    "        model: The MaskFormerForInstanceSegmentation model.\n",
    "        num_backbone_stages_to_unfreeze: Number of backbone stages to unfreeze (from the end).\n",
    "        num_pixel_decoder_layers_to_unfreeze: Number of pixel decoder layers to unfreeze (from the end).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Access the Backbone and Pixel Decoder\n",
    "    backbone = model.model.pixel_level_module.encoder.model.encoder  # Access encoder layers within backbone\n",
    "    pixel_decoder = model.model.pixel_level_module.decoder  # Access decoder\n",
    "\n",
    "    # 2. Unfreeze Backbone Stages\n",
    "    for i in range(len(backbone.layers) - num_backbone_stages_to_unfreeze, len(backbone.layers)):\n",
    "        for param in backbone.layers[i].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # 3. Unfreeze Pixel Decoder Layers\n",
    "    # The pixel decoder in MaskFormer is a single module, not a stack of layers.\n",
    "    # So, we directly unfreeze its parameters.\n",
    "    for param in pixel_decoder.parameters():  # Corrected line\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 4. Verify Unfreezing (Optional)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Unfrozen: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e41fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_gradually(model, num_backbone_stages_to_unfreeze=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c375e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreeze pixel decoder\n",
    "for param in pixel_decoder.parameters():  # Corrected line\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: MaskFormerForInstanceSegmentation,\n",
    "    dataloader: DataLoader,\n",
    "    preprocessor: AutoImageProcessor,\n",
    "    metric: Any,\n",
    "    id2label: dict,\n",
    "    max_batches=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the given model using the specified dataloader and computes the mean Intersection over Union (IoU).\n",
    "    ----\n",
    "    Args:\n",
    "      - model (MaskFormerForInstanceSegmentation): The trained model to be evaluated.\n",
    "      - dataloader (DataLoader): DataLoader containing the dataset for evaluation.\n",
    "      - preprocessor (AutoImageProcessor): The preprocessor used for post-processing the model outputs.\n",
    "      - metric (Any): Metric instance used for calculating IoU.\n",
    "      - id2label (dict): Dictionary mapping class ids to their corresponding labels.\n",
    "      - max_batches (int, optional): Maximum number of batches to evaluate. If None, evaluates on the entire validation dataset.\n",
    "\n",
    "    Returns:\n",
    "    float: The mean IoU calculated over the specified number of batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_iou = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(tqdm(dataloader)):\n",
    "            if max_batches and idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "\n",
    "            original_images = batch[\"original_images\"]\n",
    "            target_sizes = [\n",
    "                (image.shape[0], image.shape[1]) for image in original_images\n",
    "            ]\n",
    "\n",
    "            predicted_segmentation_maps = (\n",
    "                preprocessor.post_process_semantic_segmentation(\n",
    "                    outputs, target_sizes=target_sizes\n",
    "                )\n",
    "            )\n",
    "\n",
    "            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "            metric.add_batch(\n",
    "                references=ground_truth_segmentation_maps,\n",
    "                predictions=predicted_segmentation_maps,\n",
    "            )\n",
    "\n",
    "            running_iou += metric.compute(num_labels=len(id2label), ignore_index=0)[\n",
    "                \"mean_iou\"\n",
    "            ]\n",
    "            num_batches += 1\n",
    "\n",
    "    mean_iou = running_iou / num_batches\n",
    "    return mean_iou\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: MaskFormerForInstanceSegmentation,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    preprocessor: AutoImageProcessor,\n",
    "    metric: AutoImageProcessor,\n",
    "    id2label: dict,\n",
    "    num_epochs=20,\n",
    "    learning_rate=5e-5,\n",
    "    log_interval=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the MaskFormer model for semantic segmentation over a specified number of epochs and evaluates it on a validation set.\n",
    "    ----\n",
    "    Args:\n",
    "      - model (MaskFormerForInstanceSegmentation): The model to be trained.\n",
    "      - train_dataloader (DataLoader): DataLoader for the training data.\n",
    "      - val_dataloader (DataLoader): DataLoader for the validation data.\n",
    "      - preprocessor (AutoImageProcessor): The preprocessor used for preparing the data.\n",
    "      - metric (Any): Metric instance used for calculating performance metrics.\n",
    "      - id2label (dict): Dictionary mapping class IDs to their corresponding labels.\n",
    "      - num_epochs (int): Number of epochs to train the model.\n",
    "      - learning_rate (float): Learning rate for the optimizer.\n",
    "      - log_interval (int): Interval (in number of batches) at which to log training progress.\n",
    "\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Current epoch: {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_samples = 0\n",
    "\n",
    "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            batch_size = batch[\"pixel_values\"].size(0)\n",
    "            running_loss += loss.item()\n",
    "            num_samples += batch_size\n",
    "\n",
    "            if idx % log_interval == 0 and idx > 0:\n",
    "                print(f\"Iteration {idx} - loss: {running_loss/num_samples}\")\n",
    "\n",
    "            optimizer.step()\n",
    "        val_mean_iou = evaluate_model(\n",
    "            model, val_dataloader, preprocessor, metric, id2label, max_batches=6\n",
    "        )\n",
    "        print(f\"Validation Mean IoU: {val_mean_iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    preprocessor,\n",
    "    metric,\n",
    "    id2label,\n",
    "    num_epochs=1,\n",
    "    log_interval=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538143a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model_with_schedule(model, train_dataloader, val_dataloader, preprocessor, metric, id2label, train_schedule):\n",
    "    output_dir = r'/content/drive/My Drive/AI Models/Mask2Former_base'  # Or your preferred output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    backbone = model.model.pixel_level_module.encoder.encoder  # Access encoder layers within backbone\n",
    "    pixel_decoder = model.model.pixel_level_module.decoder  # Access decoder\n",
    "\n",
    "    #starting point is frozen pixel_decoder and backbone\n",
    "    for param in model.model.pixel_level_module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Confirm that the parameters are correctly frozen\n",
    "    for name, param in model.model.pixel_level_module.named_parameters():\n",
    "        assert not param.requires_grad\n",
    "\n",
    "    #for phase_index, phase in enumerate(train_schedule):\n",
    "    for phase_index, phase in enumerate(train_schedule[2:], start=2):\n",
    "        learning_rate = phase[\"learning_rate\"]\n",
    "        epochs = phase[\"epochs\"]\n",
    "\n",
    "        # Freeze/Unfreeze Pixel Decoder\n",
    "        for param in pixel_decoder.parameters():\n",
    "            param.requires_grad = not phase[\"pixel_decoder_freeze\"]\n",
    "\n",
    "        # Freeze/Unfreeze Backbone Layers (if needed)\n",
    "        num_backbone_unfreeze = phase[\"num_backbone_unfreeze\"]\n",
    "\n",
    "        if num_backbone_unfreeze > 0:  # Only unfreeze if num_backbone_freeze is greater than 0\n",
    "            for i in range(len(backbone.layers) - num_backbone_unfreeze, len(backbone.layers)):\n",
    "                for param in backbone.layers[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Train\n",
    "        train_model(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            preprocessor,\n",
    "            metric,\n",
    "            id2label,\n",
    "            num_epochs=epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            log_interval=100,\n",
    "        )\n",
    "\n",
    "        # Save Model\n",
    "        model_iteration = os.path.join(output_dir, f'phase_{phase_index + 1}')\n",
    "        if not os.path.exists(model_iteration):\n",
    "            os.makedirs(model_iteration)\n",
    "        model.save_pretrained(model_iteration)\n",
    "        processor.save_pretrained(model_iteration)\n",
    "\n",
    "        # Test Mean IoU\n",
    "        test_mean_iou = evaluate_model(model, test_dataloader, preprocessor, metric, id2label)\n",
    "        print(f\"Test Mean IoU (Phase {phase_index + 1}): {test_mean_iou}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schedule = [\n",
    "    {\"learning_rate\": 5e-5, \"epochs\": 15, \"pixel_decoder_freeze\": True, \"num_backbone_unfreeze\": 0},  # Phase 1\n",
    "    {\"learning_rate\": 1e-5, \"epochs\": 10, \"pixel_decoder_freeze\": False, \"num_backbone_unfreeze\": 0},  # Phase 2\n",
    "    {\"learning_rate\": 5e-6, \"epochs\": 5, \"pixel_decoder_freeze\": False, \"num_backbone_unfreeze\": 1},  # Phase 3\n",
    "    {\"learning_rate\": 5e-6, \"epochs\": 5, \"pixel_decoder_freeze\": False, \"num_backbone_unfreeze\": 2},  # Phase 4\n",
    "    {\"learning_rate\": 5e-6, \"epochs\": 5, \"pixel_decoder_freeze\": False, \"num_backbone_unfreeze\": 3},  # Phase 5\n",
    "    {\"learning_rate\": 5e-6, \"epochs\": 5, \"pixel_decoder_freeze\": False, \"num_backbone_unfreeze\": 4},  # Phase 6\n",
    "    # Add more phases as needed...\n",
    "]\n",
    "train_model_with_schedule(model, train_dataloader, val_dataloader, preprocessor, metric, id2label, train_schedule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
